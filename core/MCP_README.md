# JetsonMind MCP Integration

## Overview
JetsonMind provides a complete AI inference platform through the Model Context Protocol (MCP), enabling any MCP-compatible client to access advanced edge AI capabilities.

## üöÄ Quick Start

### For Q CLI Users
```bash
# MCP server is automatically configured
q chat
> Generate text using strategic thinking mode for project planning
```

### For MCP Client Developers
```bash
# Server location
/home/petr/jetson/core/run_mcp_server.sh

# Test the server
cd /home/petr/jetson/core
source mcp_env/bin/activate
python3 test_mcp_client_examples.py
```

## üìã What's Available

### üß† AI Capabilities
- **6 AI Models**: From lightweight (gpt2-small) to powerful (llama-7b)
- **3 Thinking Modes**: immediate, strategic, future
- **Agent Compatibility**: OpenAI-format responses
- **Batch Processing**: Multiple prompts efficiently

### üíæ Memory Management
- **3 Memory Tiers**: RAM (6GB), SWAP (7GB), Storage (unlimited)
- **Hot Loading**: Load/unload models without restart
- **Intelligent Placement**: Automatic tier selection
- **Memory Optimization**: LRU eviction and optimization strategies

### üîß System Features
- **Real-time Monitoring**: Memory usage, model status, performance
- **Session Management**: Persistent agent conversations
- **OpenAPI Compatibility**: REST API specification
- **Performance Metrics**: Detailed system analytics

## üìö Documentation

### For Users
- **[MCP_CLIENT_GUIDE.md](MCP_CLIENT_GUIDE.md)** - Complete tool documentation with examples
- **[MCP_QUICK_REFERENCE.md](MCP_QUICK_REFERENCE.md)** - Quick reference card

### For Developers
- **[test_mcp_client_examples.py](test_mcp_client_examples.py)** - Interactive examples
- **[mcp_inference_enhanced.py](mcp_inference_enhanced.py)** - Server implementation
- **[model_manager.py](model_manager.py)** - Advanced model management

## üõ†Ô∏è Available Tools

| Tool | Purpose | Key Features |
|------|---------|--------------|
| `generate_text` | Text generation | Thinking modes, model selection |
| `list_models` | Model library | Capabilities, tiers, specifications |
| `get_model_info` | Model details | Size, capabilities, loading status |
| `select_optimal_model` | AI recommendations | Intelligent model selection |
| `manage_model_loading` | Hot loading | Load/unload with tier control |
| `get_memory_status` | Memory monitoring | Real-time usage across tiers |
| `hot_swap_models` | Model swapping | Instant model replacement |
| `batch_inference` | Batch processing | Multiple prompts efficiently |
| `create_agent_session` | Agent sessions | Persistent conversations |
| `get_system_status` | Health monitoring | System status and capabilities |

## üéØ Usage Examples

### Basic Text Generation
```json
// Simple generation
{"prompt": "Hello world"}

// Strategic thinking
{"prompt": "Plan a software project", "thinking_mode": "strategic"}

// Agent mode
{"prompt": "Help me code", "agent_mode": true}
```

### Model Management
```json
// Load model to RAM for speed
{"action": "load", "model_name": "llama-7b", "force_tier": "RAM"}

// Hot swap for different tasks
{"source_model": "gpt2-small", "target_model": "bert-large"}

// Check memory usage
{} // get_memory_status
```

### System Monitoring
```json
// System health
{} // get_system_status

// Performance metrics
{} // get_performance_metrics

// Memory optimization
{"strategy": "balanced"} // optimize_memory
```

## üß† Model Selection Guide

### For Speed (RAM Tier)
- **gpt2-small** (0.5GB): Quick responses, simple tasks
- **gpt2-medium** (1.5GB): Balanced performance
- **bert-large** (1.3GB): Classification, embeddings

### For Quality (SWAP Tier)
- **gpt-j-6b** (6.0GB): Complex reasoning, thinking tasks
- **llama-7b** (7.0GB): Agent conversations, instruction following

### Thinking-Capable Models
- **gpt-j-6b**: Strategic and future thinking modes
- **llama-7b**: Agent compatibility and reasoning

## üí° Best Practices

### Performance Optimization
1. **Load frequent models to RAM tier**
2. **Use batch processing for multiple prompts**
3. **Monitor memory usage regularly**
4. **Hot swap models based on workload**

### Quality Enhancement
1. **Use strategic mode for complex tasks**
2. **Choose thinking-capable models for analysis**
3. **Create agent sessions for conversations**
4. **Select optimal models with AI recommendations**

### Memory Management
1. **Check memory status before loading large models**
2. **Cache unused models to storage**
3. **Use optimization strategies (balanced/aggressive)**
4. **Monitor system resources**

## üîß Troubleshooting

### Common Issues

**Model Loading Fails**
```json
// Check memory first
{} // get_memory_status

// Free space if needed
{"strategy": "aggressive"} // optimize_memory

// Try different tier
{"action": "load", "model_name": "model", "force_tier": "SWAP"}
```

**Performance Issues**
```json
// Check system status
{} // get_system_status

// Move to faster tier
{"action": "load", "model_name": "model", "force_tier": "RAM"}

// Optimize memory
{"strategy": "balanced"}
```

**Memory Errors**
```json
// Check usage
{} // get_memory_status

// Unload unused models
{"action": "unload", "model_name": "unused", "to_storage": true}

// Aggressive cleanup
{"strategy": "aggressive"}
```

## üöÄ Advanced Features

### Hot Model Swapping
Instantly replace models without system restart:
```json
{"source_model": "current_model", "target_model": "new_model", "target_tier": "RAM"}
```

### Intelligent Memory Management
Automatic tier selection and LRU eviction:
- Models automatically placed in optimal tiers
- Least recently used models evicted when memory full
- Storage tier provides unlimited caching

### Thinking Modes
Enhanced reasoning capabilities:
- **Strategic**: Structured analysis with planning
- **Future**: Forward-looking predictions and trends
- **Immediate**: Fast responses for simple tasks

### Agent Compatibility
OpenAI-format responses for agent frameworks:
```json
{"prompt": "Help me", "agent_mode": true}
// Returns OpenAI chat completion format
```

## üìä System Specifications

### Memory Limits
- **RAM Tier**: 6.0GB (80% of system RAM)
- **SWAP Tier**: 7.0GB (60% of system swap)
- **Storage Tier**: Unlimited disk-based caching

### Performance
- **Model Loading**: <1s for RAM, <3s for SWAP
- **Hot Swapping**: <0.5s model replacement
- **Batch Processing**: Up to 10x efficiency gain
- **Memory Optimization**: Automatic LRU management

### Compatibility
- **MCP Protocol**: Full MCP 1.0 compatibility
- **OpenAI Format**: Agent framework compatibility
- **REST API**: OpenAPI 3.0 specification available
- **Q CLI**: Native integration with Amazon Q

---

**JetsonMind MCP Server v4.0**  
*Complete AI Platform for Edge Devices*  
*10 Tools ‚Ä¢ 6 Models ‚Ä¢ 3 Memory Tiers ‚Ä¢ Hot Loading*
