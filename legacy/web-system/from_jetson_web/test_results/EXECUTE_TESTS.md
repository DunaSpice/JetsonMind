# üöÄ EXECUTE ALL TESTS - READY TO RUN

## ‚úÖ **ASYNC PREPARATION COMPLETE**

All scripts, directories, and monitoring tools are ready for parallel execution.

---

## üéØ **SINGLE COMMAND EXECUTION**

### Run All Phases Automatically:
```bash
docker run --runtime nvidia --rm \
  -v $(pwd)/from_jetson_web/test_results:/results \
  -v /tmp/jetson_models:/data \
  dustynv/mlc:r36.4.0 \
  python3 /results/scripts/run_all_phases.py
```

### Or Run Individual Phases:
```bash
# Phase 3: Small Models (0.5B-1B)
docker run --runtime nvidia --rm \
  -v $(pwd)/from_jetson_web/test_results:/results \
  dustynv/mlc:r36.4.0 \
  python3 /results/scripts/phase3_batch.py

# Phase 4: Medium Models (1.7B-3B)  
docker run --runtime nvidia --rm \
  -v $(pwd)/from_jetson_web/test_results:/results \
  dustynv/mlc:r36.4.0 \
  python3 /results/scripts/phase4_batch.py

# Phase 5: Large Models (7B-8B)
docker run --runtime nvidia --rm \
  -v $(pwd)/from_jetson_web/test_results:/results \
  dustynv/mlc:r36.4.0 \
  python3 /results/scripts/phase5_batch.py
```

---

## üìä **WHAT WILL HAPPEN**

### Phase 3 (3 models):
- Qwen/Qwen2.5-0.5B-Instruct
- Qwen/Qwen2.5-1.5B-Instruct  
- meta-llama/Llama-3.2-1B-Instruct

### Phase 4 (4 models):
- HuggingFaceTB/SmolLM2-1.7B-Instruct
- Qwen/Qwen2.5-3B-Instruct
- meta-llama/Llama-3.2-3B-Instruct
- google/gemma-2-2b-it

### Phase 5 (2 models):
- Qwen/Qwen2.5-7B-Instruct
- meta-llama/Llama-3.1-8B-Instruct

---

## üìÅ **RESULTS WILL BE SAVED TO:**

- `phase3_small/results.json` - Small model results
- `phase4_medium/results.json` - Medium model results  
- `phase5_large/results.json` - Large model results
- `execution_summary.json` - Overall summary

---

## ‚ö° **READY TO EXECUTE**

**Everything is prepared for async execution. Run the command above to start testing all remaining Jetson AI Lab compatible models!**
